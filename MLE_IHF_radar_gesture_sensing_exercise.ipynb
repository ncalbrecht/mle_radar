{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04057fc0",
   "metadata": {},
   "source": [
    "This tutorial is part of the lecture: \"Machine Learning in Electrical Engineering and Information Technology\" at Hamburg University of Technology (TUHH).\n",
    "\n",
    "Copyright: \n",
    "Dr.-Ing. Fabian Lurz, \n",
    "Institute of High-Frequency Technology,\n",
    "21073 Hamburg, Germany,\n",
    "fabian.lurz@ieee.org,\n",
    "May, 2023\n",
    "\n",
    "**The dataset and code is provided by the Institute of High-Frequency Technology (IHF) of the Hamburg University of Technology exclusively for use in the course \"Machine Learning in Electrical Engineering and Information Technology\". Please note that the use of the dataset and/or code for other purposes is not permitted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023810a9",
   "metadata": {},
   "source": [
    "# Tutorial 3: Building and Training Artificial Neural Networks for Radar-Based Gesture Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149610d",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ccf4d4",
   "metadata": {},
   "source": [
    "Welcome to this interactive session in which we will train artificial neural networks for radar-based hand gesture recognition. \n",
    "\n",
    "The gestures were recorded with an Infineon BGT60TR13C 60 GHz FMCW radar system. The system was lying on the table and looking upwards.\n",
    "<img src=\"img/recording_setup.png\" alt=\"Recording setup\" style=\"width: 250px;\"/>\n",
    "\n",
    "Since this tutorial focuses on machine learning rather than radar signal processing, we use already preprocessed spectrograms of the gestures. In the example below the x-axis represents the time\n",
    "dimension and the y-axis the physical size which is stated above the respective spectrograms for the gesture 'circle clockwise'. \n",
    "<img src=\"img/exemplary_spectrogram_circle_clockwise.png\" alt=\"Exemplary spectrogram\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31da694",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "- You should modify the marked parts of the program and you can execute the function section directly. To do this, add the code between ### start ### and ### end ###.\n",
    "- To execute a function section, you should select it and then press `STRG` + `ENTER`.\n",
    "- Python 3 is used for this course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f079d",
   "metadata": {},
   "source": [
    "## Import of relevant Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a40165",
   "metadata": {},
   "source": [
    "At the beginning of each project the most important packages are imported, which are needed in the further course. This includes [Matplotlib](https://matplotlib.org) to generate graphics, [Numpy](https://numpy.org) to perform vector calculations and of course the [Keras](https://keras.io) package from [TensorFlow](https://www.tensorflow.org) to create the neural networks.\n",
    "\n",
    "**Task:** Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import dropout, nn\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420fa74",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c376a21",
   "metadata": {},
   "source": [
    "The dataset consists of the preprocessed gestures `samples.npy` and the corresponding labels `labels.npy`. It is stored in the folder `./gestures_preprocessed/` and can be loaded with the function np.load(file).\n",
    "\n",
    "\n",
    "**Task:** Import the dataset. <br>\n",
    "*Note:* The dataset is approx. 60 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ####\n",
    "samples = \n",
    "labels = \n",
    "### end ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0267c",
   "metadata": {},
   "source": [
    "When loading the dataset, two NumPy arrays are returned.</br>\n",
    "* The variable `samples` contains contains the pre-processed radar data of the gestures.\n",
    "* The varialbe `labels` consists of numeric digits representing the 10 different gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab811d",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41171989",
   "metadata": {},
   "source": [
    "Let us first take a closer look at the data set together.\n",
    "* With `type(variable)` the type of the variable can be displayed. \n",
    "* and `len(variable)` gives the number of entries\n",
    "\n",
    "**Task**: Test both commands to determine the type and length of *samples*, *labels*, and *start_end_idxs*. Furthermore, check if the number of labels corresponds to the number of data points. (Reminder: with the `print(...)` command multiple outputs of a cell can be printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ####\n",
    "...\n",
    "...\n",
    "### end ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba9f33",
   "metadata": {},
   "source": [
    "The Numpy library has implemented the `variable.shape` attribute, which gives more precise information about the dimensions of the vectors.<br>\n",
    "**Task:** Use the `shape` attribute for both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b00225",
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ####\n",
    "dim_samples = ...\n",
    "dim_labels = ...\n",
    "\n",
    "print(...)\n",
    "### end ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7e6c5",
   "metadata": {},
   "source": [
    "As you can easily see from the \"**Dimensions labels: (2500,)**\", the dataset contains 2500 gestures. \n",
    "\n",
    "To understand how exactly these are stored we need to take a closer look at the \"**Dimensions samples: (2500, 4, 60, 32)**\"\n",
    "* `2500`: The number of gestures in the dataset\n",
    "* `4`: We are using already pre-processed radar signals that are: Range, Doppler, Azimuth and Elevation spectrograms (2D-images)\n",
    "* `60`: The width of each image: Time axis. Each gesture is recorded with 60 frames. As the frame repetition time is 30 ms, the duration of each gesture is max. 1.8 seconds.\n",
    "* `32` The height of each image: Range / Doppler / Azimuth / Elevation axis\n",
    "\n",
    "So for each of our 2500 gestures, we have 4 pictures with 60*32 pixels. In each image 60 represents the time axis and 32 the Range / Doppler / Azimuth / Elevation axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8cfc43",
   "metadata": {},
   "source": [
    "## Number of the individual gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99a441",
   "metadata": {},
   "source": [
    "Furthermore, it is interesting to see how many gestures of each number are present in the data set. For this purpose, the frequency of the respective classes is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb884ed6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Number, frequency_in_dataset = np.unique(labels, return_counts=True)\n",
    "plt.bar(Number, frequency_in_dataset)\n",
    "plt.xlabel('Number in the image')\n",
    "plt.ylabel('Quantity in the data set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fff1c",
   "metadata": {},
   "source": [
    "As you can see, there are 10 different types of gesture. All gestures are present with the same amount. This is ideal for our use and does not require any further adjustments. Stronger deviations should otherwise be considered here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116a1f3",
   "metadata": {},
   "source": [
    "## Gesture set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff48a1c",
   "metadata": {},
   "source": [
    "The different gestures are:\n",
    "* `left_right`: hand swipe from the left to the right side\n",
    "* `right_left`: hand swipe from the right to the left side\n",
    "* `top_down`: hand swipe from the top to the bottom\n",
    "* `down_top`: hand swipe from the bottom to the top\n",
    "* `circle_clockwise`: periodic circular hand movement in clockwise direction\n",
    "* `circle_anticlockwise`: periodic circular hand movement in anticlockwise direction\n",
    "* `forward`: hand swipe away from the body\n",
    "* `backward`: hand swipe towards the body\n",
    "* `finger_wave`: wave single fingers\n",
    "* `finger_rub`: thumb sliding over fingers\n",
    "\n",
    "<img src=\"img/gestures.png\" alt=\"Gestures\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_set = [\"left_right\", \"right_left\", \"top_down\", \"down_top\", \"circle_clockwise\", \"circle_anticlockwise\", \"forward\", \"backward\", \"finger_wave\", \"finger_rub\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6f45f",
   "metadata": {},
   "source": [
    "## Illustrate the individual gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e2e2d",
   "metadata": {},
   "source": [
    "To get a better understanding of the dataset we will plot some gestures. To do so try the Matplotlib function `plt.imshow(dataset)`. \n",
    "\n",
    "**Hints:**\n",
    "* To get a single number from the data set, it can be selected by the \"slicing\" with `dataset[index]`.\n",
    "* Slicing also works multi-dimensionally. For example, with `dataset[gesture_index,:,:]` you can retrieve all data for a single gesture selected with gesture_index.\n",
    "* To simply swap x- and y- axis numPy provides the function [numpy.transpose()](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html) can be used\n",
    "* To add a caption to the plot you can use plt.title('text')\n",
    "* To draw multiple plots at once [plt.subplot()](https://www.w3schools.com/python/matplotlib_subplot.asp) can be used\n",
    "* Remember: The dimension of samples are: (2500, 4, 60, 32)\n",
    "\n",
    "**Task**: Plot some numbers for the `samples`. Modify gesture_index to display all eight different gestures one after the other and check them for plausibility \n",
    "\n",
    "To check your output: gesture_index = 1211 should produce the exact same diagrams as shown in the introduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_index = 1  # The gesture to plot, modify this value to set through the array \n",
    "## start \n",
    "\n",
    "# plot Range, Doppler, Azimuth, Elevation for the given gesture_index\n",
    "\n",
    "# print() the label of the given gesture_index\n",
    "\n",
    "## stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081eef0",
   "metadata": {},
   "source": [
    "## Preparation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb538d",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93f912",
   "metadata": {},
   "source": [
    "Typically, the weights between the neurons of a neural network are initiated in a range between zero and one.  \n",
    "Now, if values between, e.g., 0-255 (from our dataset so far) are given to the ANN as input, it must adjust to the range in the first steps of training. While this is possible, it unnecessarily increases training time and may make the neural network less reliable.\n",
    "\n",
    "**Task** Check if the data in our dataset needs to be normalized, if so perform the normalization, or if they are already between 0 - 1.\n",
    "\n",
    "**Hint** There are existing numpy function which output min and max value of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of the data values\n",
    "### start ###\n",
    "samples_min = ...\n",
    "samples_max = ...\n",
    "### end ###\n",
    "\n",
    "if samples_min < 0 or samples_max > 1:\n",
    "    print(\"Needs normalization!\")\n",
    "else:\n",
    "    print(\"No normalization needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b156655",
   "metadata": {},
   "source": [
    "## Splitting into test and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb8da3e",
   "metadata": {},
   "source": [
    "Finally, it is still necessary to divide the dataset into a test and an evaluation dataset. Our algorithm is trained on the training dataset. Then, the evaluation dataset is predicted with the adapted neural network to evaluate the algorithm.\n",
    "\n",
    "For this the already existing function `train_test_split` of `Scikit-Learn` is first imported and executed.<br>\n",
    "The data set and the labels are specified as arguments of the function. Afterwards it is defined how many percent of the data are selected pseudo-randomly into the test data set. Furthermore, it is possible to define by specifying a `random_state` that the same numbers are selected each time this function is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02814ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired split ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Reshape the samples to (2500, 4*60*32)\n",
    "reshaped_samples = samples.reshape(samples.shape[0], -1)\n",
    "\n",
    "# Split the reshaped samples into train and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(reshaped_samples, labels, train_size=train_ratio, random_state=42)\n",
    "\n",
    "# Reshape train_samples and test_samples back to their original shape\n",
    "train_data = train_data.reshape(train_data.shape[0], 4, 60, 32)\n",
    "test_data = test_data.reshape(test_data.shape[0], 4, 60, 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee50f79",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9a103",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9876b2",
   "metadata": {},
   "source": [
    "The spectrogram architecture is a three layered convolutional neural network (CNN) using rectified linear unit (ReLU) activation, batch normalization and max pooling followed by two fully connected layers with, at the end, 10 output neurons for the 10 gesture classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d72f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(4, 32, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32, 32, 3),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32, 64, 3),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(640, 64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 10),\n",
    ")\n",
    "\n",
    "# Check / Display the model \n",
    "torchsummary.summary(model, (4, 60, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epochs and batch size\n",
    "num_epochs = 50\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b662ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_data and test_data to PyTorch tensors and set the data type\n",
    "train_data_t = torch.from_numpy(train_data).float()\n",
    "test_data_t = torch.from_numpy(test_data).float()\n",
    "\n",
    "# Convert train_labels and test_labels to PyTorch tensors and set the data type\n",
    "train_labels_t = torch.from_numpy(train_labels)\n",
    "test_labels_t = torch.from_numpy(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfda33",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bd139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the train and test losses and accuracies\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Shuffle and divide the training data into batches\n",
    "    permutation = torch.randperm(train_data_t.size(0))\n",
    "    shuffled_data = train_data_t[permutation]\n",
    "    shuffled_labels = train_labels_t[permutation]\n",
    "    num_batches = len(train_data_t) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # Extract the current batch\n",
    "        batch_data = shuffled_data[i * batch_size : (i + 1) * batch_size]\n",
    "        batch_labels = shuffled_labels[i * batch_size : (i + 1) * batch_size]\n",
    "        \n",
    "        # Convert batch_labels to torch.long data type\n",
    "        batch_labels = batch_labels.long()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        # Backward propagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        total_train += batch_labels.size(0)\n",
    "        correct_train += (predicted_train == batch_labels).sum().item()\n",
    "    \n",
    "    # Calculate average training loss and accuracy\n",
    "    epoch_loss = running_loss / num_batches\n",
    "    train_accuracy = correct_train / total_train * 100\n",
    "    \n",
    "    # Evaluation on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_data_t)\n",
    "        _, predicted_test = torch.max(test_outputs, 1)\n",
    "        correct_test = (predicted_test == test_labels_t).sum().item()\n",
    "        test_accuracy = correct_test / len(test_labels_t) * 100\n",
    "    \n",
    "    # Store loss and accuracy values\n",
    "    train_losses.append(epoch_loss)\n",
    "    test_losses.append(loss.item())\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    # Print training and test information\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Test Loss: {test_losses[-1]:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54cc742",
   "metadata": {},
   "source": [
    "When the training starts, the current values for the *loss* and *accuracy* are displayed for the test and training set respectively. So it is easy to follow the progress of the training already during the training and to notice possible errors early. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4216f8",
   "metadata": {},
   "source": [
    "## Evaluation of the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e520c",
   "metadata": {},
   "source": [
    "### Train and Test losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089afb7b",
   "metadata": {},
   "source": [
    "Now we will display the training history. For this purpose we have written the training progress in the variables `train_losses`, `test_losses`, `train_accuracies`, and `test_accuracies` in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338617ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and test losses\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot train and test accuracies\n",
    "plt.figure()\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e20f8",
   "metadata": {},
   "source": [
    "### Evaluation on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61efe4",
   "metadata": {},
   "source": [
    "Now we can use the network to predict the part of the data that was split off for evaluation. This is made possible with the attribute `prediction = model.predict(X)` of the class `model`. As a result we get the output of the Softmax layer. An example is shown with the code of the cell under the task. \n",
    "\n",
    "**Task:** Check the trained network with the `X_evaluation` dataset and change the input_index to test different results. \n",
    "\n",
    "**Hint:** To easily find misclassified samples, check out the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7583f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the index of the input to predict\n",
    "input_index = 1\n",
    "\n",
    "# Convert the input data to a tensor, remove extra dimensions\n",
    "input_tensor = torch.squeeze(test_data_t[input_index])\n",
    "\n",
    "# Perform the prediction\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor.unsqueeze(0))\n",
    "    _, predicted_label = torch.max(output, 1)\n",
    "\n",
    "# Convert the predicted label to a scalar value\n",
    "predicted_label = predicted_label.item()\n",
    "\n",
    "\n",
    "# Plot the gesture \n",
    "# Create a figure with 2x2 subplots\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.suptitle(f\"Predicted Label: {gesture_set[int(predicted_label)]}, \\n True Label: {gesture_set[int(test_labels[int(input_index)])]}\")  # Add an overall title to the figure\n",
    "\n",
    "# Subplot 1: Range over time\n",
    "axs[0, 0].imshow(test_data[input_index, 0, :, :].transpose())\n",
    "axs[0, 0].set_title('Range over time')\n",
    "\n",
    "# Subplot 2: Doppler over time\n",
    "axs[0, 1].imshow(test_data[input_index, 1, :, :].transpose())\n",
    "axs[0, 1].set_title('Doppler over time')\n",
    "\n",
    "# Subplot 3: Azimuth over time\n",
    "axs[1, 0].imshow(test_data[input_index, 2, :, :].transpose())\n",
    "axs[1, 0].set_title('Azimuth over time')\n",
    "\n",
    "# Subplot 4: Elevation over time\n",
    "axs[1, 1].imshow(test_data[input_index, 3, :, :].transpose())\n",
    "axs[1, 1].set_title('Elevation over time')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "for i in range(len(test_data)):\n",
    "    input_tensor = torch.squeeze(test_data_t[i])\n",
    "    \n",
    "    # Perform the prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor.unsqueeze(0))\n",
    "        _, predicted_label = torch.max(output, 1)\n",
    "    \n",
    "    # Convert the predicted label to a scalar value\n",
    "    predicted_label = predicted_label.item()\n",
    "    \n",
    "    # Get the true label\n",
    "    true_label = test_labels[i]\n",
    "    \n",
    "    # Check if the prediction is correct\n",
    "    if predicted_label != true_label:\n",
    "        wrong_indices.append(i)\n",
    "\n",
    "# Print the indices of misclassified classified samples\n",
    "print(\"Indices of the misclassified samples:\", wrong_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eee8be",
   "metadata": {},
   "source": [
    "### Confusions Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3e61f",
   "metadata": {},
   "source": [
    "Scikit-Learn offers the possibility to generate a Confusions Matrix. In this matrix the predictions are compared with the true values. On the diagonal is the accuracy with which the respective number is predicted. Next to the diagonal is the percentage of incorrectly predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29774928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training and testing, get the predictions on the test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_data_t)\n",
    "    _, test_predictions_tensor = torch.max(test_outputs.data, 1)\n",
    "    correct_test = (predicted_test == test_labels_t).sum().item()\n",
    "    test_accuracy = correct_test / len(test_labels_t) * 100\n",
    "\n",
    "# Print the accuracy of the final test results\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")    \n",
    "    \n",
    "# Convert the tensor predictions to a numpy array\n",
    "test_predictions_numpy = test_predictions_tensor.numpy()\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(test_labels, test_predictions_numpy, normalize=\"true\")\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(gesture_set)\n",
    "\n",
    "# Plot the confusion matrix with normalized numbers\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.xticks(np.arange(len(gesture_set)), gesture_set, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(len(gesture_set)), gesture_set)\n",
    "\n",
    "# Add text annotations of normalized numbers\n",
    "thresh = confusion_mat.max() / 2.0\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        plt.text(j, i, f'{confusion_mat[i, j]:.2f}', ha='center', va='center',\n",
    "                 color='white' if confusion_mat[i, j] > thresh else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57831429",
   "metadata": {},
   "source": [
    "## Optimization and further tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459cae8",
   "metadata": {},
   "source": [
    "**Congratulations**, <br>\n",
    "The foundation stone has been laid and you have trained an artificial neural network that is capable of recognizing hand gestures using a highly-integrated 60 GHz FMCW radar sensor. <br><br>\n",
    "**However,** the system is not perfect yet and you can start now to optimize the parameters to further improve the prediction. \n",
    "\n",
    "**Task:** You can also run some further tests to check the quality of the network and the importance of the data. This can also help you understanding what the network learns from the data. As ideas:\n",
    "* If you delete the azimuth and elevation angle `samples[:,2,:,:] = 0` and/or `samples[:,3,:,:] = 0` in the samples. What do you expect? Try it out and verify your considerations. \n",
    "* If you delete the azimuth and elevation angle `test_data[:,2,:,:] = 0` and/or `test_data[:,3,:,:] = 0` only in the test data. What do you expect? Try it out and verify your considerations. \n",
    "\n",
    "* Have you wondered why the **validation loss** is sometimes < than the **training loss**? Or why the test accuracy is sometimes *better* that the train accuracy? If yes, you might find [this explanation (https://twitter.com/aureliengeron/status/1110839223878184960)](https://twitter.com/aureliengeron/status/1110839223878184960) very helpful. <br>\n",
    "Go back and test what happens to the model when you remove (comment out) the Droput layer. Write down the maximum test accuracy beforehand and compare them. What do you notice?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
